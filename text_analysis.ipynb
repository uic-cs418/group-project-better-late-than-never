{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ee211-43e0-41f8-b20f-530bcf9089a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import sklearn\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "from scipy import sparse\n",
    "from IPython.display import display, Latex, Markdown\n",
    "warnings.filterwarnings('ignore')\n",
    "import data_cleaning as dc\n",
    "import review_score_analysis as rs\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('tagsets_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9e59f1-13aa-4a46-a130-b714b8a6eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Processing\n",
    "\n",
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    posMapping = {\n",
    "    # \"First_Letter by nltk.pos_tag\":\"POS_for_lemmatizer\"\n",
    "        \"N\":'n',\n",
    "        \"V\":'v',\n",
    "        \"J\":'a',\n",
    "        \"R\":'r'\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Create regex to catch URLs\n",
    "    url_regex = re.compile(r'''(\n",
    "        (?:https?://)?        ## Optionally match http:// or https://\n",
    "        (?:www\\.)?            ## Optionally match www.\n",
    "        [\\w.-]+\\.\\w+          ## Match multiple domains (example.com or sub.domain.co.uk)\n",
    "        (?:[/?#][^\\s]*)?      ## Optionally match paths, queries, or fragments\n",
    "    )''', re.VERBOSE)\n",
    "    \n",
    "    ### Process string\n",
    "    # Remove URLs\n",
    "    text = url_regex.sub(\"\", text).strip()\n",
    "    # Remove all ('s) e.g. she's -> she\n",
    "    text = re.sub(\"'s\", \"\", text).strip()\n",
    "    # Omit other apostrophes e.g. don't -> dont\n",
    "    text = re.sub(\"'\", \"\", text).strip()\n",
    "    # swap all other punctuation with ' '\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    # Set to lowercase\n",
    "    text = str.lower(text)\n",
    "    \n",
    "    ### Process tokens\n",
    "    # tokenize string\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    # Tag tokens\n",
    "    tokenized_text = nltk.pos_tag(tokenized_text)\n",
    "    # lemmatize tokens, converting pos tags based on mappings above\n",
    "    lemmatized_tokens = []\n",
    "    for word,tag in tokenized_text:\n",
    "        try:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=posMapping[tag[0]])\n",
    "        except KeyError:\n",
    "            # Anything not caught by posMapping dict has pos 'n'\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "        # except:\n",
    "        #     # Ignore other exceptions\n",
    "        #     continue\n",
    "        lemmatized_tokens.append(lemma)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process() function.\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
    "                        the output from process() function. Other columns are unaffected.\n",
    "    \"\"\"\n",
    "    df['text'] = df['text'].apply(process)\n",
    "    return df\n",
    "\n",
    "### Feature Construction\n",
    "def create_features(processed_reviews, stop_words):\n",
    "    \"\"\" creates the feature matrix using the processed review text\n",
    "    Inputs:\n",
    "        processed_reviews: pd.DataFrame: processed reviews read from train/test  file, containing the column 'text'\n",
    "        stop_words: list(str): stop_words by nltk stopwords (after processing)\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "            we need this to tranform test reviews in the same way as train reviews\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    # Convert processed tweets text values to list of strings, with one tweet per string\n",
    "    reviews_list = processed_reviews[\"text\"].apply(lambda x: ' '.join(x)).tolist()\n",
    "\n",
    "    # Learn vocabulary and idf, return document-term matrix\n",
    "    tfidf = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "        min_df=2, lowercase=False, stop_words=stop_words\n",
    "    )\n",
    "    X = tfidf.fit_transform(reviews_list)\n",
    "\n",
    "    return tfidf, X\n",
    "\n",
    "\n",
    "def create_labels(avg_scores_df):\n",
    "    \"\"\" creates the class labels from avg_review_score\n",
    "    Inputs:\n",
    "        avg_scores_df: pd.DataFrame: reviews read from training df, containing the column 'avg_review_score'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): series of class labels \n",
    "        1 for restaurants with avg_review_score >= 4.5\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    # Apply vectorized  operation to score restaurants\n",
    "    label_series = (avg_scores_df['avg_review_score'] >= 4.5).astype(int)\n",
    "\n",
    "    return label_series\n",
    "\n",
    "### Classification\n",
    "def learn_classifier(X_train, y_train, kernel):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_labels()\n",
    "        kernel: str: kernel function to be used with classifier. [linear|poly|rbf|sigmoid]\n",
    "    Outputs:\n",
    "        sklearn.svm.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = sklearn.svm.SVC(kernel=kernel)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_validation: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_validation: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    # Run classification of predicted political party based on each tweet\n",
    "    predicted_labels = classifier.predict(X_validation)\n",
    "\n",
    "    # Calculate accuracy of predictions\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_validation, predicted_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "class MajorityLabelClassifier():\n",
    "    \"\"\"\n",
    "    A classifier that predicts the mode of training labels\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your parameter here\n",
    "        \"\"\"\n",
    "        # Declare uninitialized mode\n",
    "        self.mode = np.nan\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Implement fit by taking training data X and their labels y and finding the mode of y\n",
    "        i.e. store your learned parameter\n",
    "        \"\"\"\n",
    "        # Convert y to a series, if it is not already\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "        # Count number of values in each label\n",
    "        counts = y.value_counts()\n",
    "        # Set mode to index (i.e. label) of most frequently occuring value\n",
    "        self.mode = counts.idxmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Implement to give the mode of training labels as a prediction for each data instance in X\n",
    "        return labels\n",
    "        \"\"\"\n",
    "        predicted_labels = []\n",
    "        for value in X:\n",
    "            predicted_labels.append(self.mode)\n",
    "\n",
    "\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759db37f-52ae-43a6-b78e-b2c7f321e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "chunk_size = 100_000\n",
    "restaurants_df = dc.load(\"data/filtered_restaurants.json\", chunk_size)\n",
    "reviews_df = dc.load(\"data/filtered_reviews.json\", chunk_size)\n",
    "avg_scores_df = rs.calculate_average_review_score(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee99de2-f185-48bb-8d38-d2bff4180f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4371248 entries, 0 to 4371247\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   review_id         object        \n",
      " 1   user_id           object        \n",
      " 2   business_id       object        \n",
      " 3   stars             int64         \n",
      " 4   useful            int64         \n",
      " 5   funny             int64         \n",
      " 6   cool              int64         \n",
      " 7   text              object        \n",
      " 8   date              datetime64[ns]\n",
      " 9   avg_review_score  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(4)\n",
      "memory usage: 333.5+ MB\n"
     ]
    }
   ],
   "source": [
    "avg_scores_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d964a9fe-d97b-42f0-916c-53bd56f5bdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [if, you, decide, to, eat, here, just, be, awa...\n",
       "1    [family, diner, have, the, buffet, eclectic, a...\n",
       "2    [wow, yummy, different, delicious, our, favori...\n",
       "3    [cute, interior, and, owner, give, u, tour, of...\n",
       "4    [i, be, a, long, term, frequent, customer, of,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Build and Test Model\n",
    "\n",
    "# This takes a while, so we're gonna use 20K reviews for testing\n",
    "processed_reviews = process_all(reviews_df[0:20000])\n",
    "processed_reviews[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8633e1e4-113c-4393-93f5-10009a4e1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "processed_stopwords = list(np.concatenate([process(word) for word in stopwords]))\n",
    "(tfidf, X) = create_features(processed_reviews, processed_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15866fd-2655-44a4-b50e-a2edbfea03b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "19995    0\n",
       "19996    0\n",
       "19997    0\n",
       "19998    0\n",
       "19999    0\n",
       "Name: avg_review_score, Length: 20000, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Create a label for each review by merging avg score with reviews df\n",
    "y = create_labels(avg_scores_df[0:20000])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba9ab031-1134-47ff-bab8-593e6aef7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against majoritylabelclassifier\n",
    "\n",
    "baselineClf = MajorityLabelClassifier()\n",
    "# Use fit and predict methods to get predictions and compare it with the true labels y\n",
    "baselineClf.fit(X,y)\n",
    "predicted_labels = baselineClf.predict(X)\n",
    "\n",
    "baseline = sklearn.metrics.accuracy_score(y, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25970e20-55ea-4cbd-a4e4-0f08a71450a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994\n"
     ]
    }
   ],
   "source": [
    "# TODO: learn and evaluate classifier\n",
    "review_classifier = learn_classifier(X, y, 'poly')\n",
    "\n",
    "accuracy = evaluate_classifier(review_classifier, X, y)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs418",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
