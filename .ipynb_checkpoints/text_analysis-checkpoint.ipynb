{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1ee211-43e0-41f8-b20f-530bcf9089a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /home/chasty2/nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import sklearn\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "from scipy import sparse\n",
    "from IPython.display import display, Latex, Markdown\n",
    "warnings.filterwarnings('ignore')\n",
    "import data_cleaning as dc\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('tagsets_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9e59f1-13aa-4a46-a130-b714b8a6eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Processing\n",
    "\n",
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    posMapping = {\n",
    "    # \"First_Letter by nltk.pos_tag\":\"POS_for_lemmatizer\"\n",
    "        \"N\":'n',\n",
    "        \"V\":'v',\n",
    "        \"J\":'a',\n",
    "        \"R\":'r'\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Create regex to catch URLs\n",
    "    url_regex = re.compile(r'''(\n",
    "        (?:https?://)?        ## Optionally match http:// or https://\n",
    "        (?:www\\.)?            ## Optionally match www.\n",
    "        [\\w.-]+\\.\\w+          ## Match multiple domains (example.com or sub.domain.co.uk)\n",
    "        (?:[/?#][^\\s]*)?      ## Optionally match paths, queries, or fragments\n",
    "    )''', re.VERBOSE)\n",
    "    \n",
    "    ### Process string\n",
    "    # Remove URLs\n",
    "    text = url_regex.sub(\"\", text).strip()\n",
    "    # Remove all ('s) e.g. she's -> she\n",
    "    text = re.sub(\"'s\", \"\", text).strip()\n",
    "    # Omit other apostrophes e.g. don't -> dont\n",
    "    text = re.sub(\"'\", \"\", text).strip()\n",
    "    # swap all other punctuation with ' '\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    # Set to lowercase\n",
    "    text = str.lower(text)\n",
    "    \n",
    "    ### Process tokens\n",
    "    # tokenize string\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    # Tag tokens\n",
    "    tokenized_text = nltk.pos_tag(tokenized_text)\n",
    "    # lemmatize tokens, converting pos tags based on mappings above\n",
    "    lemmatized_tokens = []\n",
    "    for word,tag in tokenized_text:\n",
    "        try:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=posMapping[tag[0]])\n",
    "        except KeyError:\n",
    "            # Anything not caught by posMapping dict has pos 'n'\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "        # except:\n",
    "        #     # Ignore other exceptions\n",
    "        #     continue\n",
    "        lemmatized_tokens.append(lemma)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process() function.\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
    "                        the output from process() function. Other columns are unaffected.\n",
    "    \"\"\"\n",
    "    df['text'] = df['text'].apply(process)\n",
    "    return df\n",
    "\n",
    "### Feature Construction\n",
    "def create_features(processed_tweets, stop_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: processed tweets read from train/test csv file, containing the column 'text'\n",
    "        stop_words: list(str): stop_words by nltk stopwords (after processing)\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "            we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    # Convert processed tweets text values to list of strings, with one tweet per string\n",
    "    tweets_list = processed_tweets[\"text\"].apply(lambda x: ' '.join(x)).tolist()\n",
    "\n",
    "    # Learn vocabulary and idf, return document-term matrix\n",
    "    tfidf = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "        min_df=2, lowercase=False, stop_words=stop_words\n",
    "    )\n",
    "    X = tfidf.fit_transform(tweets_list)\n",
    "\n",
    "    return tfidf, X\n",
    "\n",
    "def create_labels(processed_tweets):\n",
    "    \"\"\" creates the class labels from screen_name\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: tweets read from train file, containing the column 'screen_name'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    # Define mapping\n",
    "    label_dict = {\n",
    "        'realDonaldTrump': 0,\n",
    "        'mike_pence': 0,\n",
    "        'GOP': 0,\n",
    "        'HillaryClinton': 1,\n",
    "        'timkaine': 1, \n",
    "        'TheDemocrats': 1\n",
    "    }\n",
    "\n",
    "    # Apply mapping, default to NaN (screen name not found)\n",
    "    label_series = processed_tweets['screen_name'].map(lambda x: label_dict.get(x, np.nan))\n",
    "\n",
    "    return label_series\n",
    "\n",
    "### Classification\n",
    "def learn_classifier(X_train, y_train, kernel):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_labels()\n",
    "        kernel: str: kernel function to be used with classifier. [linear|poly|rbf|sigmoid]\n",
    "    Outputs:\n",
    "        sklearn.svm.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = sklearn.svm.SVC(kernel=kernel)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_validation: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_validation: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    # Run classification of predicted political party based on each tweet\n",
    "    predicted_labels = classifier.predict(X_validation)\n",
    "\n",
    "    # Calculate accuracy of predictions\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_validation, predicted_labels)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759db37f-52ae-43a6-b78e-b2c7f321e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test ML task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
